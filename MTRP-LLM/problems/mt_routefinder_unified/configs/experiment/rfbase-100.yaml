# @package _global_


######################################################################
# NOTE: do not use this directly, as it is the base config
######################################################################

# Override defaults: take configs from relative path
defaults:
  - override /model: routefinder.yaml
  - override /env: mtvrp.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml
  # - override /logger: null # comment this line to enable logging
  - override /logger: wandb.yaml

seed: 69420

env:
  generator_params:
    num_loc: 100
    variant_preset: "all" # NOTE: original is "single_feat", but we use all in our setting

  val_file: [cvrp/val/100.npz, ovrp/val/100.npz, ovrpb/val/100.npz, ovrpbl/val/100.npz,
              ovrpbltw/val/100.npz, ovrpbtw/val/100.npz, ovrpl/val/100.npz, ovrpltw/val/100.npz,
              ovrptw/val/100.npz, vrpb/val/100.npz, vrpl/val/100.npz, vrpbltw/val/100.npz,
              vrpbtw/val/100.npz, vrpbl/val/100.npz, vrpltw/val/100.npz, vrptw/val/100.npz,
              cvrp/val/50.npz, vrptw/val/50.npz, # generalization
  ]


  val_dataloader_names: [cvrp100, ovrp100, ovrpb100, ovrpbl100,
                          ovrpbltw100, ovrpbtw100, ovrpl100, ovrpltw100,
                          ovrptw100, vrpb100, vrpl100, vrpbltw100,
                          vrpbtw100, vrpbl100, vrpltw100, vrptw100,
                          cvrp50, vrptw50,]

  test_file: [cvrp/test/100.npz, ovrp/test/100.npz, ovrpb/test/100.npz, ovrpbl/test/100.npz,
              ovrpbltw/test/100.npz, ovrpbtw/test/100.npz, ovrpl/test/100.npz, ovrpltw/test/100.npz,
              ovrptw/test/100.npz, vrpb/test/100.npz, vrpl/test/100.npz, vrpbltw/test/100.npz,
              vrpbtw/test/100.npz, vrpbl/test/100.npz, vrpltw/test/100.npz, vrptw/test/100.npz,
              cvrp/test/50.npz, vrptw/test/50.npz, # generalization
  ]

  test_dataloader_names: ${env.val_dataloader_names}


# Logging: we use Wandb in this case
logger:
  wandb:
    project: "routefinder"
    tags: ["rfbase", "${env.name}"]
    group: "${env.generator_params.variant_preset}-${env.generator_params.num_loc}"
    name: "rfbase"

# Note that we use 100k per epoch, so total is 1000 epochs instead of 10k
# However, due to resource constraints (and time), we consider training to 100 epochs
# https://github.com/FeiLiu36/MTNCO/blob/c5b3b2b8158a2262cc61238b26041ece1594e7e7/MTPOMO/POMO/train_n100.py#L66
model:
  batch_size: 256
  # note: batch size is a list corresponding to num of datasets
  val_batch_size: 128
  test_batch_size: ${model.val_batch_size}
  train_data_size: 100_000
  # note: data size is a list corresponding to num of datasets
  val_data_size: 64
  test_data_size: 64 # NOTE: unused if provided by env
  optimizer_kwargs:
    lr: 3e-4 # NOTE: we will be using 3e-4 from now on
    weight_decay: 1e-6
  lr_scheduler:
    "MultiStepLR"
  lr_scheduler_kwargs:
    milestones: [270, 295]
    gamma: 0.1
  normalize_reward: "exponential"
  norm_operation: "div"
  alpha: 0.25

trainer:
  # max_epochs: 1000 (full run as per the paper)
  max_epochs: 300 #100 # 100 epochs ~ 8hrs on 1x3090, so we allow 24 hrs

# Easier default under logs/ directory
callbacks:
  model_checkpoint:
    dirpath: logs/${logger.wandb.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}/checkpoints
    monitor: "val/reward/cvrp100"