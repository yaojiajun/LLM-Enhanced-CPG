def heuristics_v1(current_distance_matrix: torch.Tensor, delivery_node_demands: torch.Tensor, current_load: torch.Tensor, time_windows: torch.Tensor,
current_time: torch.Tensor, ovrp_attribute: torch.Tensor, pickup_node_demands: torch.Tensor, current_length: torch.Tensor) -> torch.Tensor:
    # cvrp_scores
    # Compute the normalized distance-based heuristic score matrix with added diversity through randomness
    normalized_distance_scores = -current_distance_matrix / torch.max(current_distance_matrix) + torch.randn_like(
    current_distance_matrix) * 0.7

    # Compute the demand-based heuristic score matrix with emphasis on high-demand nodes and enhanced randomness
    demand_scores = (delivery_node_demands.unsqueeze(0) - current_load.unsqueeze(1)) * 0.8 + torch.max(
        delivery_node_demands) / 2 + torch.randn_like(current_distance_matrix) * 0.5

    # Introduce increased randomness for exploration with higher noise level for improved diversity
    enhanced_noise = torch.randn_like(current_distance_matrix) * 2.0

    # Combine the different heuristic scores with diversified strategies for balanced exploration
    cvrp_scores = normalized_distance_scores + demand_scores + enhanced_noise


    # vrptw_scores
    pomo_size, num_nodes = cvrp_scores.shape

    # Extract earliest and latest time windows
    earliest_windows = time_windows[:, 0].unsqueeze(0)  # (1, N+1)
    latest_windows = time_windows[:, 1].unsqueeze(0)   # (1, N+1)

    # Compute estimated arrival times
    estimated_arrival = current_time.unsqueeze(1) + cvrp_scores  # (pomo_size, N+1)

    # Calculate waiting time and overtime penalties
    waiting_time = torch.clamp(earliest_windows - estimated_arrival, min=0)
    over_time_penalty = torch.clamp(estimated_arrival - latest_windows, min=0)

    # Calculate penalty scaling based on overtime presence
    penalty_scaling = (1 + 2 * over_time_penalty.gt(0).float())
    time_compensation = waiting_time + over_time_penalty * penalty_scaling

    # Final VRPTW scores
    vrptw_scores = time_compensation

    #vrpb_scores
    vehicle_capacity = 100.0

    # Intensive computations in vectorized form
    non_zero_demands = pickup_node_demands > 0

    # Calculate the used capacity for all pickups
    used_capacity = torch.sum(pickup_node_demands.unsqueeze(0) * non_zero_demands.float(), dim=1)

    # Calculate remaining capacity
    remaining_capacity = vehicle_capacity - used_capacity.unsqueeze(1)

    # Calculate backhaul compensation based on remaining capacity
    vrpb_compensation = torch.where(
        non_zero_demands,
        (remaining_capacity - pickup_node_demands) * non_zero_demands.float(),
        torch.zeros_like(cvrp_scores)
    )

    # Ensure compensation is clamped to be helpful, based on excess capacity
    vrpb_compensation = torch.clamp(vrpb_compensation, min=0)

    # Compute final vrpb_scores with broadcasting
    vrpb_scores = vrpb_compensation

    #vrpl_scores
    pomo_size, n_plus_1 = cvrp_scores.shape

    # Create a mask where the current_length is less than or equal to indices indicating feasible visits
    feasible_mask = (current_length.unsqueeze(1) > 0)  # Shape: (pomo_size, 1)

    # Compute duration criticality, amplifying scores for feasible paths
    vrpl_compensation = torch.where(feasible_mask, 1 / (current_length.unsqueeze(1) + 1e-6), torch.zeros_like(cvrp_scores))  # Avoid division by zero

    # Final computation of vrpl_scores
    vrpl_scores = vrpl_compensation

    #ovrp_scores

    ovrp_scores = torch.zeros_like(cvrp_scores)

    overall_scores=cvrp_scores+vrptw_scores+vrpb_scores+vrpl_scores+ovrp_scores

    return overall_scores